# Anomaly detection for JSON documents

## Abstract
Standard statistical methods can be used for anomaly detection of one dimensional real valued data. The multidimensional nature of JSON documents makes anomaly detection more difficult. This README proposes a two stage algorithm for the anomaly detection of JSON documents. The first stage of the algorithm uses random matrix dimensionality reduction to vectorize a JSON document into a fixed length vector (JSON document vector). The second stage of the algorithm uses a neural network autoencoder to determine how surprising (autoencoder error) the JSON document vector is. Simple statistical analysis can then be used for determining which JSON documents the user should be alerted to.

## Background
* [Random projection](https://en.wikipedia.org/wiki/Random_projection)
* [Anomaly detection with autoencoders](http://philipperemy.github.io/anomaly-detection/)

## Vectorization
### Converting a JSON document into a vector
The typical approach for converting a document into a vector is to count words. Each entry in the vector corresponds to the count for a particular word. In order to capture more document struct word pairs could be counted instead. JSON documents have explicit struct that should be captured in the computed vector. For example the below JSON:
```json
{
 "a": [
  {"a": "aa"},
	{"b": "bb"}
 ],
 "b": [
  {"a": "aa"},
	{"b": "bb"}
 ]
}
```
would be converted into the below 12 "words" using a recursive algorithm:
1. a a aa
2. a aa
3. aa
4. a b bb
5. b bb
6. bb
7. b a aa
8. a aa
9. aa
10. b b bb
11. b bb
12. bb
The 8 unique words are ["a a aa", "a aa", "aa", "a b bb", "b bb", "bb", "b a aa", "b b bb"], and their vector is [1, 2, 2, 1, 2, 2, 1, 1]. For all possible JSON documents the vector would be very large, so an algorithm is needed for compressing this vector.

### Random matrix dimensionality reduction
Dimensionality reduction compresses a large vector into a smaller vector. This is done by multiplying a vector by a matrix. [Principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA) is the algorithm typically used, but it doesn't scale well for larger vectors. For very large vectors a random matrix can be used instead of a matrix generated by PCA. The random matrix is filled with (-1, 1, 0) with respective odds (1/6, 1/6, 4/6). For the vector from the previous section the matrix would look like: [0 1 0 0 -1 0 0 1; 1 0 0 -1 0 1 0 0]. This matrix would reduce the 8 entry vector down to a 2 entry vector. As an optimization the matrix columns can be generated on the fly using a random number generator seeded with the hash of a particular word. Addition can then be used to replace multiplication.

The code for the vectorizer can be found [here](https://github.com/pointlander/anomaly/blob/master/vectorizer.go).

## Anomaly detection with autoencoders
An autoencoding neural network isn't trained with labeled data, instead it is trained to output the input vector. The standard autoencoder has three layers. The top and bottom layers are the same size, and the middle layer is typically more narrow than the top and bottom layers. The narrow middle layer creates an information bottleneck. It is possible to compute an autoencoder error metric for a particular JSON document vector. This "surprise" metric is computed by inputing the JSON document vector into the neural network and then computing the [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error) at the output. The neural network can then be trained on the JSON document vector, so the neural network isn't surprised by similar JSON document vectors in the future.

## Verification
Verification is accomplished by generating random JSON documents from a gaussian random variable and feeding them into the anomaly detection algorithm. The below graph shows the distribution of autoencoder error computed from random JSON documents:

![Graph 1 autoencoder error distribution](graph_1_autoencoder_error_distribution.png?raw=true)

As should be expected graph 1 appears to be gaussian. Another test implemented feeds the below two JSON documents into the autoencoder after the autoencoder has been trained on 1000 random JSON documents:
```json
{
 "alfa": [
  {"alfa": "1"},
	{"bravo": "2"}
 ],
 "bravo": [
  {"alfa": "3"},
	{"bravo": "4"}
 ]
}
```
```json
{
 "a": [
  {"a": "aa"},
	{"b": "bb"}
 ],
 "b": [
  {"a": "aa"},
	{"b": "bb"}
 ]
}
```
The second JSON document is more similar to the randomly generated JSON documents than the first JSON document. This idea is tested 100 times by changing the random seed used to generate the 1000 random JSON documents. The first JSON document has greater autoencoder error than the second JSON document 98 out of 100 trials. This shows that the anomaly detection algorithm isn't a random number generator. One final test is performed by computing the average [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) for each 1000 randomly generated JSON documents. The below graph 3 shows that autoencoder error correlates with average cosine similarity:

![Graph 3 autoencoder error vs average similarity](graph_3_autoencoder_error_vs_average_similarity.png?raw=true)

The below two graphs show autoencoder error and average similarity are not correlated through time:

![Graph 4 autoencoder error vs time](graph_4_autoencoder_error.png?raw=true)

![Graph 5 average similarity vs time](graph_5_average_similarity.png?raw=true)

## Conclusion
An anomaly detection engine has been demonstrated. The algorithm is made up of two components: a vectorizer and an autocoding neural network. After vectorization the algorithm has a fixed cost for determining the autoencoder error. The JSON document vector can then be used to train the autoencoder thus storing the vector for future comparisons. The autoencoder error is equivalent to computing the average cosine similarity for a given JSON document.

## Future work
A number of modifications are possible for the autoencoder. One possible change is to use the TanH [activation function](https://en.wikipedia.org/wiki/Activation_function). Initial experiments with the TanH activation function failed. Another change is to use cosine similarity as the loss function for the neural network. One final idea is a recurrent neural network could be used to replace both the document vectorizer and feed forward autoencoder.
